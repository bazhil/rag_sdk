"""
–ú–æ–¥—É–ª—å –¥–ª—è –≤–µ–±-–ø–æ–∏—Å–∫–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º DuckDuckGo.
–û—Å–Ω–æ–≤–∞–Ω –Ω–∞ —Å—Ç–∞—Ç—å–µ: https://huggingface.co/learn/cookbook/multiagent_rag_system
"""

import sys
import os
from typing import List, Dict, Any, Optional
import asyncio
from urllib.parse import urlparse

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'llm_manager'))

from .config import settings

# Global LLM instance
_llm_instance = None


def _get_llm():
    """–ü–æ–ª—É—á–∞–µ—Ç –∏–ª–∏ —Å–æ–∑–¥–∞–µ—Ç —ç–∫–∑–µ–º–ø–ª—è—Ä LLM."""
    global _llm_instance
    if _llm_instance is None:
        from llm_manager.llm_factory import create_llm
        _llm_instance = create_llm()
    return _llm_instance


class WebSearchManager:
    """
    –ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è –≤–µ–±-–ø–æ–∏—Å–∫–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º DuckDuckGo.
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫, –∏–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é.
    """
    
    def __init__(self):
        print("[WEB_SEARCH] Initializing WebSearchManager...")
        self.results_count = settings.web_search_results_count
        self.max_retries = settings.web_search_max_retries
        print(f"[WEB_SEARCH] Results count: {self.results_count}")
        print(f"[WEB_SEARCH] Max retries: {self.max_retries}")
    
    def _search_duckduckgo_html(self, query: str) -> List[Dict[str, Any]]:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ –≤ DuckDuckGo —á–µ—Ä–µ–∑ HTML (fallback –º–µ—Ç–æ–¥).
        
        Args:
            query: –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
        """
        import requests
        from bs4 import BeautifulSoup
        import urllib.parse
        import time
        import random
        
        try:
            print(f"[WEB_SEARCH] HTML search for: '{query}'")
            
            # –°–æ–∑–¥–∞–µ–º —Å–µ—Å—Å–∏—é –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è cookies
            session = requests.Session()
            
            # –°–ø–∏—Å–æ–∫ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö User-Agent –¥–ª—è —Ä–æ—Ç–∞—Ü–∏–∏
            user_agents = [
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15',
                'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            ]
            
            results = []
            
            for attempt in range(self.max_retries):
                try:
                    # –ó–∞–¥–µ—Ä–∂–∫–∞ –ø–µ—Ä–µ–¥ –ø–æ–ø—ã—Ç–∫–æ–π (—É–≤–µ–ª–∏—á–∏–≤–∞—é—â–∞—è—Å—è)
                    if attempt > 0:
                        delay = (attempt * 3) + random.uniform(1, 3)
                        print(f"[WEB_SEARCH] Waiting {delay:.1f}s before retry...")
                        time.sleep(delay)
                    
                    # –ò–º–∏—Ç–∏—Ä—É–µ–º –±—Ä–∞—É–∑–µ—Ä —Å —Ä–æ—Ç–∞—Ü–∏–µ–π User-Agent
                    headers = {
                        'User-Agent': random.choice(user_agents),
                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
                        'Accept-Language': 'en-US,en;q=0.9,ru;q=0.8',
                        'Accept-Encoding': 'gzip, deflate, br',
                        'DNT': '1',
                        'Connection': 'keep-alive',
                        'Upgrade-Insecure-Requests': '1',
                        'Sec-Fetch-Dest': 'document',
                        'Sec-Fetch-Mode': 'navigate',
                        'Sec-Fetch-Site': 'none',
                        'Cache-Control': 'max-age=0'
                    }
                    
                    # –°–Ω–∞—á–∞–ª–∞ –∑–∞—Ö–æ–¥–∏–º –Ω–∞ –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è cookies
                    if attempt == 0:
                        print(f"[WEB_SEARCH] Getting cookies from main page...")
                        session.get('https://duckduckgo.com/', headers=headers, timeout=10)
                        time.sleep(random.uniform(1, 2))
                    
                    # –ö–æ–¥–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –¥–ª—è URL
                    encoded_query = urllib.parse.quote_plus(query)
                    
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—ã–π –ø–æ–∏—Å–∫, –∞ –Ω–µ html –≤–µ—Ä—Å–∏—é
                    url = f"https://duckduckgo.com/html/?q={encoded_query}&kl=us-en"
                    
                    print(f"[WEB_SEARCH] Attempt {attempt + 1}/{self.max_retries}: Requesting {url}")
                    
                    response = session.get(url, headers=headers, timeout=15, allow_redirects=True)
                    
                    print(f"[WEB_SEARCH] Response status: {response.status_code}, content length: {len(response.content)}")
                    
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.text, 'html.parser')
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                        # –í–∞—Ä–∏–∞–Ω—Ç 1: result__body (–Ω–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç)
                        search_results = soup.find_all('div', class_='result__body')
                        if not search_results:
                            # –í–∞—Ä–∏–∞–Ω—Ç 2: —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ links_main
                            search_results = soup.find_all('div', class_='links_main')
                        if not search_results:
                            # –í–∞—Ä–∏–∞–Ω—Ç 3: web-result
                            search_results = soup.find_all('div', class_='web-result')
                        
                        print(f"[WEB_SEARCH] Found {len(search_results)} result containers")
                        
                        for result in search_results[:self.results_count]:
                            try:
                                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ —Å—Å—ã–ª–∫—É
                                title_elem = result.find('a', class_='result__a')
                                if not title_elem:
                                    title_elem = result.find('a', class_='result-link')
                                if not title_elem:
                                    title_elem = result.find('a')
                                
                                if not title_elem:
                                    continue
                                
                                title = title_elem.get_text(strip=True)
                                url_link = title_elem.get('href', '')
                                
                                # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
                                snippet_elem = result.find('a', class_='result__snippet')
                                if not snippet_elem:
                                    snippet_elem = result.find('div', class_='result__snippet')
                                if not snippet_elem:
                                    snippet_elem = result.find('div', class_='snippet')
                                
                                snippet = snippet_elem.get_text(strip=True) if snippet_elem else ''
                                
                                if title and url_link:
                                    results.append({
                                        'title': title,
                                        'url': url_link,
                                        'snippet': snippet
                                    })
                                    print(f"[WEB_SEARCH] Added result: {title[:50]}...")
                                    
                            except Exception as parse_error:
                                print(f"[WEB_SEARCH] Error parsing result: {parse_error}")
                                continue
                        
                        if results:
                            print(f"[WEB_SEARCH] HTML search found {len(results)} results")
                            return results
                        else:
                            print(f"[WEB_SEARCH] No results parsed from HTML, trying next attempt...")
                    else:
                        print(f"[WEB_SEARCH] Bad status code: {response.status_code}")
                    
                except Exception as e:
                    print(f"[WEB_SEARCH] HTML search attempt {attempt + 1} failed: {e}")
            
            return results
            
        except Exception as e:
            print(f"[WEB_SEARCH] HTML search error: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def _search_duckduckgo(self, query: str) -> List[Dict[str, Any]]:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ –≤ DuckDuckGo —Å fallback –Ω–∞ HTML –º–µ—Ç–æ–¥.
        
        Args:
            query: –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
        """
        try:
            from duckduckgo_search import DDGS
            import time
            
            print(f"[WEB_SEARCH] Searching DuckDuckGo for: '{query}'")
            
            results = []
            
            # –ü—Ä–æ–±—É–µ–º API –º–µ—Ç–æ–¥
            for attempt in range(2):  # –¢–æ–ª—å–∫–æ 2 –ø–æ–ø—ã—Ç–∫–∏ –¥–ª—è API
                try:
                    ddgs = DDGS(timeout=15)
                    search_results = list(ddgs.text(
                        keywords=query,
                        max_results=self.results_count
                    ))
                    
                    for result in search_results:
                        results.append({
                            'title': result.get('title', ''),
                            'url': result.get('href', ''),
                            'snippet': result.get('body', ''),
                        })
                    
                    if results:
                        print(f"[WEB_SEARCH] API method: found {len(results)} results")
                        return results
                        
                except Exception as search_error:
                    print(f"[WEB_SEARCH] API attempt {attempt + 1}/2 failed: {search_error}")
                    if attempt < 1:
                        time.sleep(3)
                    continue
            
            # –ï—Å–ª–∏ API –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª, –ø—Ä–æ–±—É–µ–º HTML –º–µ—Ç–æ–¥
            print(f"[WEB_SEARCH] API method failed, trying HTML fallback...")
            results = self._search_duckduckgo_html(query)
            
            if not results:
                print(f"[WEB_SEARCH] All methods failed, no results found")
            
            return results
            
        except Exception as e:
            print(f"[WEB_SEARCH] Error during DuckDuckGo search: {e}")
            import traceback
            traceback.print_exc()
            
            # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞ —á–µ—Ä–µ–∑ HTML
            try:
                return self._search_duckduckgo_html(query)
            except:
                return []
    
    def _fetch_webpage_content(self, url: str, max_length: int = 5000) -> Optional[str]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã.
        
        Args:
            url: URL –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã
            max_length: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∏–∑–≤–ª–µ–∫–∞–µ–º–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
            
        Returns:
            –¢–µ–∫—Å—Ç–æ–≤–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            import requests
            from bs4 import BeautifulSoup
            
            print(f"[WEB_SEARCH] Fetching content from: {url}")
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏
            for script in soup(['script', 'style', 'nav', 'footer', 'header']):
                script.decompose()
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
            text = soup.get_text(separator=' ', strip=True)
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
            if len(text) > max_length:
                text = text[:max_length] + "..."
            
            print(f"[WEB_SEARCH] Extracted {len(text)} characters")
            return text
            
        except Exception as e:
            print(f"[WEB_SEARCH] Error fetching webpage {url}: {e}")
            return None
    
    async def search_and_summarize(
        self,
        query: str,
        fetch_content: bool = True
    ) -> Dict[str, Any]:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ –∏ —Å–æ–∑–¥–∞–µ—Ç —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.
        
        Args:
            query: –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            fetch_content: –Ω—É–∂–Ω–æ –ª–∏ –∏–∑–≤–ª–µ–∫–∞—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø–æ–∏—Å–∫–∞ –∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–µ–π
        """
        print(f"\n[WEB_SEARCH] ========== WEB SEARCH REQUEST ==========")
        print(f"[WEB_SEARCH] Query: {query}")
        print(f"[WEB_SEARCH] Fetch content: {fetch_content}")
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
        search_results = self._search_duckduckgo(query)
        
        if not search_results:
            return {
                'query': query,
                'results': [],
                'summary': '''–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –Ω–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É.

**–í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:**
- DuckDuckGo –≤—Ä–µ–º–µ–Ω–Ω–æ –æ–≥—Ä–∞–Ω–∏—á–∏–ª –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã (rate limiting)
- –ü—Ä–æ–±–ª–µ–º—ã —Å —Å–µ—Ç–µ–≤—ã–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ–º
- –ó–∞–ø—Ä–æ—Å –±—ã–ª –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω –∞–Ω—Ç–∏–±–æ—Ç–æ–º

**–ß—Ç–æ –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å:**
1. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –∑–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç
2. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç—ã–µ –∏–ª–∏ –æ–±—â–∏–µ –∑–∞–ø—Ä–æ—Å—ã
3. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –¥–ª—è –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–µ–π

üí° **–°–æ–≤–µ—Ç:** –ü–µ—Ä–µ–∫–ª—é—á–∏—Ç–µ—Å—å –Ω–∞ —Ä–µ–∂–∏–º RAG –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏.''',
                'sources_count': 0
            }
        
        # –ï—Å–ª–∏ –Ω—É–∂–Ω–æ, –∏–∑–≤–ª–µ–∫–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü
        enriched_results = []
        for result in search_results:
            enriched_result = result.copy()
            
            if fetch_content and result.get('url'):
                content = self._fetch_webpage_content(result['url'])
                if content:
                    enriched_result['content'] = content
            
            enriched_results.append(enriched_result)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é —Å –ø–æ–º–æ—â—å—é LLM
        summary = await self._generate_summary(query, enriched_results)
        
        print(f"[WEB_SEARCH] Summary generated: {len(summary)} chars")
        print(f"[WEB_SEARCH] ========================================\n")
        
        return {
            'query': query,
            'results': search_results,  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –±–∞–∑–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–∑ content
            'summary': summary,
            'sources_count': len(search_results)
        }
    
    async def _generate_summary(
        self,
        query: str,
        results: List[Dict[str, Any]]
    ) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ —Å –ø–æ–º–æ—â—å—é LLM.
        
        Args:
            query: –∏—Å—Ö–æ–¥–Ω—ã–π –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            results: —Å–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ —Å —Å–æ–¥–µ—Ä–∂–∏–º—ã–º
            
        Returns:
            –°—É–º–º–∞—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
        """
        print("[WEB_SEARCH] Generating summary with LLM...")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
        context_parts = []
        for i, result in enumerate(results, 1):
            part = f"**–ò—Å—Ç–æ—á–Ω–∏–∫ {i}: {result['title']}**\n"
            part += f"URL: {result['url']}\n"
            part += f"–û–ø–∏—Å–∞–Ω–∏–µ: {result['snippet']}\n"
            
            if 'content' in result and result['content']:
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                content = result['content'][:2000]
                part += f"–°–æ–¥–µ—Ä–∂–∏–º–æ–µ: {content}\n"
            
            context_parts.append(part)
        
        context = "\n\n---\n\n".join(context_parts)
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è LLM
        prompt = f"""–ù–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤–µ–±-–ø–æ–∏—Å–∫–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤—å –ø–æ–¥—Ä–æ–±–Ω—ã–π –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

–í–û–ü–†–û–° –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø:
{query}

–†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–û–ò–°–ö–ê:
{context}

–¢–†–ï–ë–û–í–ê–ù–ò–Ø –ö –û–¢–í–ï–¢–£:
1. –î–∞–π —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π, –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
2. –ò—Å–ø–æ–ª—å–∑—É–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –≤—Å–µ—Ö —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
3. –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π –æ—Ç–≤–µ—Ç —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ –∏ —Å–ø–∏—Å–∫–∞–º–∏
4. –í –∫–æ–Ω—Ü–µ –¥–æ–±–∞–≤—å —Ä–∞–∑–¥–µ–ª "–ò—Å—Ç–æ—á–Ω–∏–∫–∏" —Å–æ —Å—Å—ã–ª–∫–∞–º–∏ –Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã
5. –£–∫–∞–∑—ã–≤–∞–π –Ω–æ–º–µ—Ä–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ [1], [2] –∏ —Ç.–¥.
6. –ü–∏—à–∏ –Ω–∞ —Ç–æ–º –∂–µ —è–∑—ã–∫–µ, —á—Ç–æ –∏ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê:
[–¢–≤–æ–π –ø–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç–≤–µ—Ç —Å —Å—Å—ã–ª–∫–∞–º–∏ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∏]

## –ò—Å—Ç–æ—á–Ω–∏–∫–∏:
1. [–ù–∞–∑–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ 1] - URL
2. [–ù–∞–∑–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ 2] - URL
...

–û–¢–í–ï–¢:"""
        
        # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç LLM
        llm = _get_llm()
        summary = await llm.get_response("", prompt)
        
        return summary


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –º–µ–Ω–µ–¥–∂–µ—Ä–∞
_web_search_manager = None


def get_web_search_manager() -> WebSearchManager:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–∏–Ω–≥–ª—Ç–æ–Ω —ç–∫–∑–µ–º–ø–ª—è—Ä WebSearchManager."""
    global _web_search_manager
    if _web_search_manager is None:
        _web_search_manager = WebSearchManager()
    return _web_search_manager

