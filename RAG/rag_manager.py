import sys
import os
import re
from typing import List, Dict, Any, Optional

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'llm_manager'))

from .vector_store import VectorStore
from .embeddings import EmbeddingModel
from .document_processor import DocumentProcessor
from .config import settings
from .language_detector import get_language_detector
from .pdf_generator import get_pdf_generator


class RAGManager:
    
    def __init__(self):
        print("[RAG_MANAGER] Initializing RAGManager components...")
        self.vector_store = VectorStore()
        self.embedding_model = EmbeddingModel()
        self.document_processor = DocumentProcessor()
        self.language_detector = get_language_detector()
        self._llm = None
        print("[RAG_MANAGER] RAGManager initialized with auto language detection")
        
    async def initialize(self):
        print("[RAG_MANAGER] Starting initialization...")
        await self.vector_store.connect()
        print("[RAG_MANAGER] Vector store connected")
        self.embedding_model.load()
        print("[RAG_MANAGER] Embedding model loaded")
        print("[RAG_MANAGER] Initialization complete")
        
    async def close(self):
        print("[RAG_MANAGER] Closing connections...")
        await self.vector_store.close()
        print("[RAG_MANAGER] Closed successfully")
        
    def _get_llm(self):
        if self._llm is None:
            print("[RAG_MANAGER] Loading LLM manager...")
            from llm_factory import get_llm_manager
            self._llm = get_llm_manager()
            print(f"[RAG_MANAGER] LLM manager loaded: {self._llm.__class__.__name__}")
        return self._llm
        
    async def add_document(self, file_path: str, filename: str) -> int:
        print(f"\n[RAG_MANAGER] ========== ADD DOCUMENT START ==========")
        print(f"[RAG_MANAGER] File: {filename}")
        print(f"[RAG_MANAGER] Path: {file_path}")
        
        text = await self.document_processor.extract_text_from_file(file_path, filename)
        print(f"[RAG_MANAGER] Extracted text: {len(text)} characters")
        
        chunks = self.document_processor.split_text_into_chunks(text)
        print(f"[RAG_MANAGER] Split into {len(chunks)} chunks")
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        document_lang = self.language_detector.detect_document_language(chunks)
        print(f"[RAG_MANAGER] Auto-detected document language: {document_lang or 'unknown'}")
        
        embeddings = self.embedding_model.encode_batch(chunks)
        print(f"[RAG_MANAGER] Generated {len(embeddings)} embeddings")
        
        file_size = os.path.getsize(file_path)
        document_id = await self.vector_store.create_document(
            filename=filename,
            file_size=file_size,
            metadata={
                'chunks_count': len(chunks),
                'language': document_lang  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —è–∑—ã–∫ –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            }
        )
        print(f"[RAG_MANAGER] Created document record: ID={document_id}")
        
        prepared_chunks = self.document_processor.prepare_chunks_for_storage(chunks, embeddings)
        await self.vector_store.add_chunks(document_id, prepared_chunks)
        print(f"[RAG_MANAGER] Stored {len(prepared_chunks)} chunks in database")
        print(f"[RAG_MANAGER] ========== ADD DOCUMENT COMPLETE: ID={document_id} ==========\n")
        
        return document_id
        
    async def search(self, query: str, document_id: Optional[int] = None, limit: Optional[int] = None, min_similarity: Optional[float] = None) -> List[Dict[str, Any]]:
        limit = limit if limit is not None else settings.search_limit
        min_similarity = min_similarity if min_similarity is not None else settings.min_similarity
        print(f"[RAG_MANAGER] Search query: '{query[:100]}...' | doc_id: {document_id} | limit: {limit}")
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞ –∑–∞–ø—Ä–æ—Å–∞
        query_lang = self.language_detector.detect_language(query)
        print(f"[RAG_MANAGER] Auto-detected query language: {query_lang or 'unknown'}")
        
        # –ü–æ–ª—É—á–∞–µ–º —è–∑—ã–∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞(–æ–≤) –µ—Å–ª–∏ –∑–∞–¥–∞–Ω –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
        document_lang = None
        if document_id:
            doc = await self.vector_store.get_document(document_id)
            if doc and doc.get('metadata'):
                document_lang = doc['metadata'].get('language')
                print(f"[RAG_MANAGER] Document language: {document_lang or 'unknown'}")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –Ω—É–∂–µ–Ω –ª–∏ –ø–µ—Ä–µ–≤–æ–¥ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞
        queries_to_search = [query]  # –í—Å–µ–≥–¥–∞ –∏—â–µ–º –ø–æ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º—É –∑–∞–ø—Ä–æ—Å—É
        
        if query_lang and document_lang and query_lang != document_lang:
            # –ö—Ä–æ—Å—Å-—è–∑—ã–∫–æ–≤–æ–π –∑–∞–ø—Ä–æ—Å –æ–±–Ω–∞—Ä—É–∂–µ–Ω - –ø—Ä–æ–±—É–µ–º –ø–µ—Ä–µ–≤–µ—Å—Ç–∏
            print(f"[RAG_MANAGER] Cross-lingual search detected ({query_lang} -> {document_lang})")
            translated_query = self.language_detector.translate_text(query, document_lang)
            
            if translated_query and translated_query != query:
                queries_to_search.append(translated_query)
                print(f"[RAG_MANAGER] Will search with {len(queries_to_search)} query variants")
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º –≤–∞—Ä–∏–∞–Ω—Ç–∞–º –∑–∞–ø—Ä–æ—Å–∞
        all_results = []
        seen_chunks = set()  # –î–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
        
        for i, search_query in enumerate(queries_to_search):
            print(f"[RAG_MANAGER] Searching with query variant {i+1}/{len(queries_to_search)}")
            
            query_embedding = self.embedding_model.encode(search_query)
            print(f"[RAG_MANAGER] Generated query embedding: {len(query_embedding)} dimensions")
            
            results = await self.vector_store.search_similar(
                query_embedding=query_embedding,
                document_id=document_id,
                limit=limit * 2
            )
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –∏–∑–±–µ–≥–∞—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            for result in results:
                chunk_id = result.get('id')
                if chunk_id not in seen_chunks:
                    seen_chunks.add(chunk_id)
                    all_results.append(result)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ similarity
        all_results.sort(key=lambda x: x.get('similarity', 0), reverse=True)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π similarity
        filtered_results = [
            result for result in all_results 
            if result.get('similarity', 0) >= min_similarity
        ]
        
        print(f"[RAG_MANAGER] Search results: {len(all_results)} total ‚Üí {len(filtered_results)} after filtering (min_similarity: {min_similarity})")
        for i, result in enumerate(filtered_results[:limit], 1):
            print(f"[RAG_MANAGER]   Top {i}: {result['filename']} (similarity: {result['similarity']:.2%})")
        
        return filtered_results[:limit]
        
    async def generate_answer(self, query: str, document_id: Optional[int] = None, context_limit: Optional[int] = None) -> Dict[str, Any]:
        context_limit = context_limit if context_limit is not None else settings.search_limit
        print(f"\n{'='*80}")
        print(f"[RAG_MANAGER] ========== GENERATE ANSWER START ==========")
        print(f"[RAG_MANAGER] Query: {query}")
        print(f"[RAG_MANAGER] Document ID filter: {document_id}")
        print(f"[RAG_MANAGER] Context limit: {context_limit}")
        print(f"{'='*80}")
        
        search_results = await self.search(query, document_id, limit=context_limit)
        
        if not search_results:
            print(f"[RAG_MANAGER] WARNING: No relevant documents found")
            return {
                'answer': '–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö.',
                'sources': [],
                'context': []
            }
        
        print(f"[RAG_MANAGER] Found {len(search_results)} relevant chunks:")
        context_parts = []
        for idx, result in enumerate(search_results, 1):
            similarity = result.get('similarity', 0)
            print(f"[RAG_MANAGER]   Chunk {idx}: {result['filename']} (similarity: {similarity:.2%}, index: {result['chunk_index']})")
            context_parts.append(f"[–ò—Å—Ç–æ—á–Ω–∏–∫ {idx} - {result['filename']}]:\n{result['content']}")
        
        context = '\n\n'.join(context_parts)
        print(f"[RAG_MANAGER] Total context: {len(context)} chars, {len(context.split())} words")
        
        prompt = f"""–ù–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –æ—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

–¢–†–ï–ë–û–í–ê–ù–ò–Ø –ö –û–¢–í–ï–¢–£:
1. –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π –æ—Ç–≤–µ—Ç —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –∏ –ø–æ–¥–∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
2. –ò—Å–ø–æ–ª—å–∑—É–π –Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏ –¥–ª—è –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏–π
3. –í—ã–¥–µ–ª—è–π –∫–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –∂–∏—Ä–Ω—ã–º —à—Ä–∏—Ñ—Ç–æ–º (**—Ç–µ—Ä–º–∏–Ω**)
4. –ù–∞—á–∏–Ω–∞–π —Å –∫—Ä–∞—Ç–∫–æ–≥–æ —Ä–µ–∑—é–º–µ –≤ 1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö
5. –ì—Ä—É–ø–ø–∏—Ä—É–π —Å–≤—è–∑–∞–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ä–∞–∑–¥–µ–ª—ã
6. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, —á–µ—Ç–∫–æ —É–∫–∞–∂–∏ —á—Ç–æ –∏–∑–≤–µ—Å—Ç–Ω–æ –∏ —á–µ–≥–æ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç
7. –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –Ω–µ –¥–æ–±–∞–≤–ª—è–π –Ω–∏—á–µ–≥–æ –æ—Ç —Å–µ–±—è

–ö–û–ù–¢–ï–ö–°–¢:
{context}

–í–û–ü–†–û–°: {query}

–û–¢–í–ï–¢:"""
        
        print(f"\n[RAG_MANAGER] {'='*80}")
        print(f"[RAG_MANAGER] FULL CONTEXT SENT TO LLM:")
        print(f"[RAG_MANAGER] {'-'*80}")
        print(context)
        print(f"[RAG_MANAGER] {'-'*80}")
        print(f"[RAG_MANAGER] Prompt length: {len(prompt)} chars")
        print(f"[RAG_MANAGER] {'='*80}\n")
        
        llm = self._get_llm()
        print(f"[RAG_MANAGER] Calling LLM: {llm.__class__.__name__}")
        answer = await llm.get_response("", prompt)
        
        print(f"\n[RAG_MANAGER] {'='*80}")
        print(f"[RAG_MANAGER] FULL LLM RESPONSE:")
        print(f"[RAG_MANAGER] {'-'*80}")
        print(answer)
        print(f"[RAG_MANAGER] {'-'*80}")
        print(f"[RAG_MANAGER] Answer length: {len(answer)} chars, {len(answer.split())} words")
        print(f"[RAG_MANAGER] {'='*80}\n")
        
        sources = [
            {
                'filename': result['filename'],
                'document_id': result['document_id'],
                'chunk_index': result['chunk_index'],
                'similarity': float(result['similarity'])
            }
            for result in search_results
        ]
        
        return {
            'answer': answer,
            'sources': sources,
            'context': [result['content'] for result in search_results]
        }
        
    async def get_documents(self) -> List[Dict[str, Any]]:
        return await self.vector_store.get_documents()
        
    async def get_document(self, document_id: int) -> Optional[Dict[str, Any]]:
        return await self.vector_store.get_document(document_id)
        
    async def delete_document(self, document_id: int):
        await self.vector_store.delete_document(document_id)
    
    async def summarize_document(self, document_id: int) -> Dict[str, Any]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ (—Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é) —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞.
        
        Args:
            document_id: ID –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
            
        Returns:
            Dict —Å –ø–æ–ª—è–º–∏: summary, document_id, filename, chunk_count
        """
        print(f"\n{'='*80}")
        print(f"[RAG_MANAGER] ========== SUMMARIZE DOCUMENT START ==========")
        print(f"[RAG_MANAGER] Document ID: {document_id}")
        print(f"{'='*80}")
        
        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ
        document = await self.vector_store.get_document(document_id)
        if not document:
            raise ValueError(f"–î–æ–∫—É–º–µ–Ω—Ç —Å ID {document_id} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        print(f"[RAG_MANAGER] Document: {document['filename']}")
        print(f"[RAG_MANAGER] Chunk count: {document['chunk_count']}")
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        chunks = await self.vector_store.get_document_chunks(document_id)
        print(f"[RAG_MANAGER] Retrieved {len(chunks)} chunks")
        
        if not chunks:
            return {
                'summary': '–î–æ–∫—É–º–µ–Ω—Ç –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏.',
                'document_id': document_id,
                'filename': document['filename'],
                'chunk_count': 0
            }
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —á–∞–Ω–∫–∏ –≤ —Ç–µ–∫—Å—Ç (—Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ chunk_index)
        sorted_chunks = sorted(chunks, key=lambda x: x.get('chunk_index', 0))
        full_text = '\n\n'.join([chunk['content'] for chunk in sorted_chunks])
        
        print(f"[RAG_MANAGER] Full text length: {len(full_text)} chars, {len(full_text.split())} words")
        
        # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π, –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ N —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
        max_chars = 15000  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª—è LLM
        if len(full_text) > max_chars:
            print(f"[RAG_MANAGER] Text too long, truncating to {max_chars} chars")
            text_for_summary = full_text[:max_chars] + "\n\n[... —Ç–µ–∫—Å—Ç –æ–±—Ä–µ–∑–∞–Ω ...]"
        else:
            text_for_summary = full_text
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
        prompt = f"""–°–æ–∑–¥–∞–π –∫—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ (—Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é) —Å–ª–µ–¥—É—é—â–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞.

–¢–†–ï–ë–û–í–ê–ù–ò–Ø –ö –°–£–ú–ú–ê–†–ò–ó–ê–¶–ò–ò:
1. –ù–∞—á–Ω–∏ —Å –æ–±—â–µ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è –≤ 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö
2. –í—ã–¥–µ–ª–∏ –æ—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–º—ã –∏ —Ä–∞–∑–¥–µ–ª—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞
3. –ò—Å–ø–æ–ª—å–∑—É–π –º–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏ –¥–ª—è –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö –ø—É–Ω–∫—Ç–æ–≤
4. –í—ã–¥–µ–ª–∏ –≤–∞–∂–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –∏ –ø–æ–Ω—è—Ç–∏—è –∂–∏—Ä–Ω—ã–º —à—Ä–∏—Ñ—Ç–æ–º (**—Ç–µ—Ä–º–∏–Ω**)
5. –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
6. –°–æ—Ö—Ä–∞–Ω–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–∑–ª–æ–∂–µ–Ω–∏—è –∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ
7. –£–∫–∞–∂–∏ –∫–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã –∏–ª–∏ –∑–∞–∫–ª—é—á–µ–Ω–∏—è, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
8. –ù–µ –¥–æ–±–∞–≤–ª—è–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –∫–æ—Ç–æ—Ä–æ–π –Ω–µ—Ç –≤ —Ç–µ–∫—Å—Ç–µ

–î–û–ö–£–ú–ï–ù–¢: {document['filename']}

–¢–ï–ö–°–¢ –î–û–ö–£–ú–ï–ù–¢–ê:
{text_for_summary}

–ö–†–ê–¢–ö–û–ï –°–û–î–ï–†–ñ–ê–ù–ò–ï:"""
        
        print(f"[RAG_MANAGER] Prompt length: {len(prompt)} chars")
        
        # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç LLM
        llm = self._get_llm()
        print(f"[RAG_MANAGER] Calling LLM: {llm.__class__.__name__}")
        summary = await llm.get_response("", prompt)
        
        print(f"\n[RAG_MANAGER] {'='*80}")
        print(f"[RAG_MANAGER] SUMMARY GENERATED:")
        print(f"[RAG_MANAGER] {'-'*80}")
        print(summary)
        print(f"[RAG_MANAGER] {'-'*80}")
        print(f"[RAG_MANAGER] Summary length: {len(summary)} chars, {len(summary.split())} words")
        print(f"[RAG_MANAGER] {'='*80}\n")
        
        print(f"[RAG_MANAGER] ========== SUMMARIZE DOCUMENT COMPLETE ==========\n")
        
        return {
            'summary': summary,
            'document_id': document_id,
            'filename': document['filename'],
            'chunk_count': len(chunks)
        }
    
    async def create_referat(self, document_id: int, output_dir: str = "referats") -> Dict[str, Any]:
        """
        –°–æ–∑–¥–∞–µ—Ç —Ä–µ—Ñ–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ –¥–æ–∫—É–º–µ–Ω—Ç–∞.
        –†–µ—Ñ–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ - —ç—Ç–æ –ø–æ–¥—Ä–æ–±–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º
        –≤—Å–µ—Ö –∫–ª—é—á–µ–≤—ã—Ö –ø–æ–ª–æ–∂–µ–Ω–∏–π, –Ω–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∫—Ä–∞—â–µ–Ω–Ω—ã–π –ø–æ –æ–±—ä–µ–º—É.
        
        Args:
            document_id: ID –¥–æ–∫—É–º–µ–Ω—Ç–∞
            output_dir: –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è PDF
            
        Returns:
            Dict —Å –ø–æ–ª—è–º–∏: referat, document_id, filename, chunk_count, pdf_url, pdf_path
        """
        print(f"\n{'='*80}")
        print(f"[RAG_MANAGER] ========== CREATE REFERAT START ==========")
        print(f"[RAG_MANAGER] Document ID: {document_id}")
        print(f"{'='*80}")
        
        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ
        document = await self.vector_store.get_document(document_id)
        if not document:
            raise ValueError(f"–î–æ–∫—É–º–µ–Ω—Ç —Å ID {document_id} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        print(f"[RAG_MANAGER] Document: {document['filename']}")
        print(f"[RAG_MANAGER] Chunk count: {document['chunk_count']}")
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        chunks = await self.vector_store.get_document_chunks(document_id)
        print(f"[RAG_MANAGER] Retrieved {len(chunks)} chunks")
        
        if not chunks:
            return {
                'referat': '–î–æ–∫—É–º–µ–Ω—Ç –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ä–µ—Ñ–µ—Ä–∞—Ç–∞.',
                'document_id': document_id,
                'filename': document['filename'],
                'chunk_count': 0,
                'pdf_url': '',
                'pdf_path': ''
            }
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º —á–∞–Ω–∫–∏ –ø–æ –ø–æ—Ä—è–¥–∫—É
        sorted_chunks = sorted(chunks, key=lambda x: x.get('chunk_index', 0))
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ —á–∞—Å—Ç–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        # –ö–∞–∂–¥–∞—è —á–∞—Å—Ç—å ~10000 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        chunk_size_for_referat = 10000
        parts = []
        current_part = []
        current_length = 0
        
        for chunk in sorted_chunks:
            chunk_text = chunk['content']
            chunk_len = len(chunk_text)
            
            if current_length + chunk_len > chunk_size_for_referat and current_part:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â—É—é —á–∞—Å—Ç—å –∏ –Ω–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—É—é
                parts.append('\n\n'.join(current_part))
                current_part = [chunk_text]
                current_length = chunk_len
            else:
                current_part.append(chunk_text)
                current_length += chunk_len
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —á–∞—Å—Ç—å
        if current_part:
            parts.append('\n\n'.join(current_part))
        
        print(f"[RAG_MANAGER] Document split into {len(parts)} parts for analysis")
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é —á–∞—Å—Ç—å
        llm = self._get_llm()
        referat_parts = []
        
        for i, part in enumerate(parts, 1):
            print(f"[RAG_MANAGER] Processing part {i}/{len(parts)}")
            
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è —Ä–µ—Ñ–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ —á–∞—Å—Ç–∏
            if len(parts) == 1:
                part_info = "–≤–µ—Å—å –¥–æ–∫—É–º–µ–Ω—Ç"
            else:
                part_info = f"—á–∞—Å—Ç—å {i} –∏–∑ {len(parts)}"
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Ü–µ–ª–µ–≤–æ–π —Ä–∞–∑–º–µ—Ä —Ä–µ—Ñ–µ—Ä–∞—Ç–∞ (35% –æ—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª–∞)
            part_word_count = len(part.split())
            target_word_count = int(part_word_count * 0.35)
            min_word_count = int(part_word_count * 0.30)
            max_word_count = int(part_word_count * 0.45)
            
            prompt = f"""–°–æ–∑–¥–∞–π –ü–û–î–†–û–ë–ù–´–ô –†–ï–§–ï–†–ê–¢–ò–í–ù–´–ô –ü–ï–†–ï–í–û–î —Å–ª–µ–¥—É—é—â–µ–π —á–∞—Å—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ ({part_info}).

‚ö†Ô∏è –°–¢–†–û–ì–ò–ï –¢–†–ï–ë–û–í–ê–ù–ò–Ø –ö –û–ë–™–ï–ú–£ (–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –í–´–ü–û–õ–ù–ò):
üìä –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: ~{part_word_count} —Å–ª–æ–≤
üìä –¶–ï–õ–ï–í–û–ô —Ä–∞–∑–º–µ—Ä —Ä–µ—Ñ–µ—Ä–∞—Ç–∞: {target_word_count} —Å–ª–æ–≤ (–º–∏–Ω–∏–º—É–º {min_word_count}, –º–∞–∫—Å–∏–º—É–º {max_word_count})
üìä –≠—Ç–æ –ø—Ä–∏–º–µ—Ä–Ω–æ 35% –æ—Ç –æ–±—ä–µ–º–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª–∞

‚ùó –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û:
- –¢–≤–æ–π —Ä–µ—Ñ–µ—Ä–∞—Ç –î–û–õ–ñ–ï–ù —Å–æ–¥–µ—Ä–∂–∞—Ç—å –º–∏–Ω–∏–º—É–º {min_word_count} —Å–ª–æ–≤
- –ï—Å–ª–∏ –ø–æ–ª—É—á–∏—Ç—Å—è –º–µ–Ω—å—à–µ - —ç—Ç–æ –û–®–ò–ë–ö–ê, –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ –¥–µ—Ç–∞–ª–µ–π
- –†–µ—Ñ–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ - —ç—Ç–æ –î–ï–¢–ê–õ–¨–ù–û–ï –∏–∑–ª–æ–∂–µ–Ω–∏–µ, –ù–ï –∫—Ä–∞—Ç–∫–∞—è –≤—ã–∂–∏–º–∫–∞
- –õ—É—á—à–µ –Ω–∞–ø–∏—Å–∞—Ç—å –±–æ–ª—å—à–µ ({max_word_count} —Å–ª–æ–≤), —á–µ–º —É–ø—É—Å—Ç–∏—Ç—å –≤–∞–∂–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é

–°–¢–†–ê–¢–ï–ì–ò–Ø –ù–ê–ü–ò–°–ê–ù–ò–Ø –ü–û–î–†–û–ë–ù–û–ì–û –†–ï–§–ï–†–ê–¢–ê:
1. **–ö–∞–∂–¥—É—é –∫–æ–Ω—Ü–µ–ø—Ü–∏—é —Ä–∞—Å–∫—Ä—ã–≤–∞–π –≤ 2-4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö**, –∞ –Ω–µ –≤ –æ–¥–Ω–æ–º
2. **–î–æ–±–∞–≤–ª—è–π –ø–æ—è—Å–Ω–µ–Ω–∏—è –∫ —Ç–µ—Ä–º–∏–Ω–∞–º** - —á—Ç–æ –æ–Ω–∏ –æ–∑–Ω–∞—á–∞—é—Ç –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
3. **–í–∫–ª—é—á–∞–π –≤—Å–µ –ø—Ä–∏–º–µ—Ä—ã –∏ –¥–∞–Ω–Ω—ã–µ** –∏–∑ —Ç–µ–∫—Å—Ç–∞ - —Ü–∏—Ñ—Ä—ã, —Ñ–∞–∫—Ç—ã, —Å–ª—É—á–∞–∏
4. **–û–ø–∏—Å—ã–≤–∞–π –º–µ—Ö–∞–Ω–∏–∑–º—ã –∏ –ø—Ä–æ—Ü–µ—Å—Å—ã** –ø–æ—à–∞–≥–æ–≤–æ, –Ω–µ –æ–±—â–∏–º–∏ —Ñ—Ä–∞–∑–∞–º–∏
5. **–°–æ—Ö—Ä–∞–Ω—è–π –∞—Ä–≥—É–º–µ–Ω—Ç–∞—Ü–∏—é** - –µ—Å–ª–∏ –≤ —Ç–µ–∫—Å—Ç–µ –µ—Å—Ç—å –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏—è, –≤–∫–ª—é—á–∏ –∏—Ö
6. **–î–µ—Ç–∞–ª–∏–∑–∏—Ä—É–π —Å–ø–∏—Å–∫–∏** - –∫ –∫–∞–∂–¥–æ–º—É –ø—É–Ω–∫—Ç—É –¥–æ–±–∞–≤–ª—è–π –æ–ø–∏—Å–∞–Ω–∏–µ
7. **–¶–∏—Ç–∏—Ä—É–π –∫–ª—é—á–µ–≤—ã–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è** –∏ –ø–æ–ª–æ–∂–µ–Ω–∏—è

–°–¢–†–£–ö–¢–£–†–ê –†–ï–§–ï–†–ê–¢–ê:
- –ó–∞–≥–æ–ª–æ–≤–∫–∏ –∏ –ø–æ–¥–∑–∞–≥–æ–ª–æ–≤–∫–∏ (## –∏ ###)
- –í–≤–æ–¥–Ω—ã–µ –∞–±–∑–∞—Ü—ã –ø–µ—Ä–µ–¥ –∫–∞–∂–¥—ã–º —Ä–∞–∑–¥–µ–ª–æ–º
- –†–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã (3-5 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∫–∞–∂–¥—ã–π)
- –î–µ—Ç–∞–ª—å–Ω—ã–µ –Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏ –º–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏ —Å –ø–æ—è—Å–Ω–µ–Ω–∏—è–º–∏
- –ü—Ä–∏–º–µ—Ä—ã –∏ –¥–∞–Ω–Ω—ã–µ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
- –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –≤—ã–≤–æ–¥—ã –ø–æ –ø–æ–¥—Ä–∞–∑–¥–µ–ª–∞–º
- –ó–∞–∫–ª—é—á–µ–Ω–∏–µ –ø–æ —Ä–∞–∑–¥–µ–ª—É

–°–¢–ò–õ–¨:
- –ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–π, –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —è–∑—ã–∫
- –¢–æ—á–Ω–æ—Å—Ç—å - —Ç–æ–ª—å–∫–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ —Ç–µ–∫—Å—Ç–∞
- –ü–æ–ª–Ω–æ—Ç–∞ - –Ω–µ –ø—Ä–æ–ø—É—Å–∫–∞–π –≤–∞–∂–Ω—ã–µ –¥–µ—Ç–∞–ª–∏
- –í—ã–¥–µ–ª—è–π —Ç–µ—Ä–º–∏–Ω—ã –∂–∏—Ä–Ω—ã–º (**—Ç–µ—Ä–º–∏–Ω**)

–ò–°–•–û–î–ù–´–ô –¢–ï–ö–°–¢ ({part_info}):
{part}

–°–û–ó–î–ê–ô –ü–û–î–†–û–ë–ù–´–ô –†–ï–§–ï–†–ê–¢–ò–í–ù–´–ô –ü–ï–†–ï–í–û–î (–º–∏–Ω–∏–º—É–º {min_word_count} —Å–ª–æ–≤):"""
            
            part_referat = await llm.get_response("", prompt)
            referat_parts.append(part_referat)
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –æ–±—ä–µ–º–∞
            referat_words = len(part_referat.split())
            compression_ratio = (referat_words / part_word_count * 100) if part_word_count > 0 else 0
            print(f"[RAG_MANAGER] Part {i} processed:")
            print(f"  - Input: {part_word_count} words, {len(part)} chars")
            print(f"  - Output: {referat_words} words, {len(part_referat)} chars")
            print(f"  - Compression: {compression_ratio:.1f}% (target: 30-45%)")
            
            if referat_words < min_word_count:
                print(f"  ‚ö†Ô∏è  WARNING: Output is below minimum ({referat_words} < {min_word_count})")
        
        # –ï—Å–ª–∏ —á–∞—Å—Ç–µ–π –±–æ–ª—å—à–µ –æ–¥–Ω–æ–π, —Å–æ–∑–¥–∞–µ–º –æ–±—â—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
        if len(referat_parts) > 1:
            print(f"[RAG_MANAGER] Starting hierarchical merging of {len(referat_parts)} parts")
            
            # –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ: –æ–±—ä–µ–¥–∏–Ω—è–µ–º —á–∞—Å—Ç–∏ –≥—Ä—É–ø–ø–∞–º–∏ –ø–æ 8
            # –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –±–æ–ª—å—à–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ —É–ø–∏—Ä–∞—è—Å—å –≤ –ª–∏–º–∏—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            def chunk_list(lst, n):
                """–†–∞–∑–±–∏–≤–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –Ω–∞ –≥—Ä—É–ø–ø—ã –ø–æ n —ç–ª–µ–º–µ–Ω—Ç–æ–≤"""
                for i in range(0, len(lst), n):
                    yield lst[i:i + n]
            
            current_parts = referat_parts
            level = 1
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø–æ–∫–∞ –Ω–µ –æ—Å—Ç–∞–Ω–µ—Ç—Å—è –æ–¥–Ω–∞ —á–∞—Å—Ç—å
            while len(current_parts) > 1:
                print(f"[RAG_MANAGER] Merging level {level}: {len(current_parts)} parts")
                next_parts = []
                
                # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –≥—Ä—É–ø–ø—ã –ø–æ 8 —á–∞—Å—Ç–µ–π
                groups = list(chunk_list(current_parts, 8))
                
                for i, group in enumerate(groups, 1):
                    if len(group) == 1:
                        # –ï—Å–ª–∏ –≤ –≥—Ä—É–ø–ø–µ –æ–¥–Ω–∞ —á–∞—Å—Ç—å, –ø—Ä–æ—Å—Ç–æ –¥–æ–±–∞–≤–ª—è–µ–º –µ—ë
                        next_parts.append(group[0])
                        continue
                    
                    print(f"[RAG_MANAGER]   Merging group {i}/{len(groups)} ({len(group)} parts)")
                    
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —á–∞—Å—Ç–∏ –≥—Ä—É–ø–ø—ã
                    combined_group = '\n\n---\n\n'.join(group)
                    
                    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ —á–∞—Å—Ç—è—Ö
                    total_words = sum(len(part.split()) for part in group)
                    min_expected_words = int(total_words * 0.85)  # –ú–∏–Ω–∏–º—É–º 85% –æ—Ç —Å—É–º–º—ã —á–∞—Å—Ç–µ–π
                    
                    merge_prompt = f"""–£ —Ç–µ–±—è –µ—Å—Ç—å {len(group)} —á–∞—Å—Ç–µ–π —Ä–µ—Ñ–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞.
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å –∏—Ö –≤ –ï–î–ò–ù–´–ô –°–í–Ø–ó–ù–´–ô —Ç–µ–∫—Å—Ç –ë–ï–ó –°–û–ö–†–ê–©–ï–ù–ò–Ø —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è.

‚ö†Ô∏è –°–¢–†–û–ì–ò–ï –¢–†–ï–ë–û–í–ê–ù–ò–Ø –ö –û–ë–™–ï–ú–£:
üìä –°—É–º–º–∞—Ä–Ω—ã–π –æ–±—ä–µ–º —á–∞—Å—Ç–µ–π: ~{total_words} —Å–ª–æ–≤
üìä –ú–ò–ù–ò–ú–ê–õ–¨–ù–´–ô –æ–±—ä–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {min_expected_words} —Å–ª–æ–≤ (–Ω–µ –º–µ–Ω—å—à–µ!)
üìä –¢—ã –¥–æ–ª–∂–µ–Ω –°–û–•–†–ê–ù–ò–¢–¨ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –≤–µ—Å—å –æ–±—ä–µ–º (85%+)

‚ùó –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û - –ß–¢–û –î–ï–õ–ê–¢–¨:
‚úÖ –°–û–•–†–ê–ù–ò –≤–µ—Å—å —Ç–µ–∫—Å—Ç –∏–∑ –≤—Å–µ—Ö —á–∞—Å—Ç–µ–π
‚úÖ –ü—Ä–æ—Å—Ç–æ —É–±–µ—Ä–∏ –ø–æ–≤—Ç–æ—Ä—ã –º–µ–∂–¥—É —á–∞—Å—Ç—è–º–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)
‚úÖ –î–æ–±–∞–≤—å —Å–≤—è–∑–∫–∏ –º–µ–∂–¥—É —Ä–∞–∑–¥–µ–ª–∞–º–∏ –¥–ª—è –ø–ª–∞–≤–Ω–æ—Å—Ç–∏
‚úÖ –û–±—ä–µ–¥–∏–Ω–∏ –≤ –ª–æ–≥–∏—á–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏
‚úÖ –°–æ—Ö—Ä–∞–Ω–∏ –í–°–ï –ø—Ä–∏–º–µ—Ä—ã, –¥–∞–Ω–Ω—ã–µ, —Ç–µ—Ä–º–∏–Ω—ã, –¥–µ—Ç–∞–ª–∏

‚ùå –ù–ï –î–ï–õ–ê–ô:
‚ùå –ù–ï —Å–æ–∫—Ä–∞—â–∞–π –æ–ø–∏—Å–∞–Ω–∏—è
‚ùå –ù–ï —É–±–∏—Ä–∞–π –¥–µ—Ç–∞–ª–∏
‚ùå –ù–ï –æ–±—ä–µ–¥–∏–Ω—è–π —Ä–∞–∑–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –≤ –æ–¥–Ω—É
‚ùå –ù–ï –ø—Ä–µ–≤—Ä–∞—â–∞–π —Å–ø–∏—Å–∫–∏ –≤ –∫—Ä–∞—Ç–∫–∏–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏
‚ùå –ù–ï —É–¥–∞–ª—è–π –ø—Ä–∏–º–µ—Ä—ã –∏–ª–∏ –¥–∞–Ω–Ω—ã–µ

–¢–ï–•–ù–ò–ö–ê –û–ë–™–ï–î–ò–ù–ï–ù–ò–Ø:
1. –í–æ–∑—å–º–∏ —Ç–µ–∫—Å—Ç –∏–∑ –ø–µ—Ä–≤–æ–π —á–∞—Å—Ç–∏ –ö–ê–ö –ï–°–¢–¨
2. –î–æ–±–∞–≤—å –ø–µ—Ä–µ—Ö–æ–¥–Ω—É—é —Ñ—Ä–∞–∑—É (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
3. –î–æ–±–∞–≤—å —Ç–µ–∫—Å—Ç –∏–∑ –≤—Ç–æ—Ä–æ–π —á–∞—Å—Ç–∏ –ö–ê–ö –ï–°–¢–¨
4. –ü–æ–≤—Ç–æ—Ä–∏ –¥–ª—è –≤—Å–µ—Ö —á–∞—Å—Ç–µ–π
5. –£–±–µ—Ä–∏ —Ç–æ–ª—å–∫–æ —è–≤–Ω—ã–µ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è (–æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
6. –ü—Ä–æ–≤–µ—Ä—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤

–ß–ê–°–¢–ò –î–õ–Ø –û–ë–™–ï–î–ò–ù–ï–ù–ò–Ø:
{combined_group}

–û–ë–™–ï–î–ò–ù–ò –í –°–í–Ø–ó–ù–´–ô –¢–ï–ö–°–¢ (–º–∏–Ω–∏–º—É–º {min_expected_words} —Å–ª–æ–≤, —Å–æ—Ö—Ä–∞–Ω–∏ –≤—Å–µ –¥–µ—Ç–∞–ª–∏):"""
                    
                    merged = await llm.get_response("", merge_prompt)
                    next_parts.append(merged)
                    
                    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
                    merged_words = len(merged.split())
                    retention_ratio = (merged_words / total_words * 100) if total_words > 0 else 0
                    print(f"[RAG_MANAGER]   Group {i} merged:")
                    print(f"    - Input: {total_words} words (from {len(group)} parts)")
                    print(f"    - Output: {merged_words} words")
                    print(f"    - Retention: {retention_ratio:.1f}% (target: 85%+)")
                    
                    if merged_words < min_expected_words:
                        print(f"    ‚ö†Ô∏è  WARNING: Merged output is too short ({merged_words} < {min_expected_words})")
                
                current_parts = next_parts
                level += 1
            
            # –§–∏–Ω–∞–ª—å–Ω–∞—è —á–∞—Å—Ç—å - –¥–æ–±–∞–≤–ª—è–µ–º –≤–≤–µ–¥–µ–Ω–∏–µ –∏ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ
            base_referat = current_parts[0]
            
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–ª–æ–≤–∞ –≤ –±–∞–∑–æ–≤–æ–º —Ä–µ—Ñ–µ—Ä–∞—Ç–µ
            base_words = len(base_referat.split())
            min_final_words = base_words + 100  # –ú–∏–Ω–∏–º—É–º: –±–∞–∑–æ–≤—ã–π —Ç–µ–∫—Å—Ç + –≤–≤–µ–¥–µ–Ω–∏–µ + –∑–∞–∫–ª—é—á–µ–Ω–∏–µ
            
            final_prompt = f"""–£ —Ç–µ–±—è –µ—Å—Ç—å –ü–û–õ–ù–´–ô —Ä–µ—Ñ–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ –¥–æ–∫—É–º–µ–Ω—Ç–∞.
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –¥–æ–±–∞–≤–∏—Ç—å –í–í–ï–î–ï–ù–ò–ï –≤ –Ω–∞—á–∞–ª–æ –∏ –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï –≤ –∫–æ–Ω–µ—Ü.

‚ö†Ô∏è –°–¢–†–û–ì–û–ï –¢–†–ï–ë–û–í–ê–ù–ò–ï –ö –û–ë–™–ï–ú–£:
üìä –†–∞–∑–º–µ—Ä —Ä–µ—Ñ–µ—Ä–∞—Ç–∞: {base_words} —Å–ª–æ–≤
üìä –ú–ò–ù–ò–ú–ê–õ–¨–ù–´–ô —Ä–∞–∑–º–µ—Ä –∏—Ç–æ–≥–æ–≤–æ–≥–æ —Ç–µ–∫—Å—Ç–∞: {min_final_words} —Å–ª–æ–≤
üìä –í–µ—Å—å —Ç–µ–∫—Å—Ç —Ä–µ—Ñ–µ—Ä–∞—Ç–∞ –¥–æ–ª–∂–µ–Ω –æ—Å—Ç–∞—Ç—å—Å—è –ë–ï–ó –ò–ó–ú–ï–ù–ï–ù–ò–ô!

‚ùó –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û:
‚ùå –ù–ï —Å–æ–∫—Ä–∞—â–∞–π –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç —Ä–µ—Ñ–µ—Ä–∞—Ç–∞!
‚ùå –ù–ï –∏–∑–º–µ–Ω—è–π —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –≤ —Ä–µ—Ñ–µ—Ä–∞—Ç–µ!
‚ùå –ù–ï —É–¥–∞–ª—è–π –ø—Ä–∏–º–µ—Ä—ã, –¥–∞–Ω–Ω—ã–µ –∏–ª–∏ –¥–µ—Ç–∞–ª–∏!
‚ùå –ù–ï –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–π —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ç–µ–∫—Å—Ç!
‚úÖ –¢–û–õ–¨–ö–û –¥–æ–±–∞–≤—å –≤–≤–µ–¥–µ–Ω–∏–µ –∏ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ

–ß–¢–û –î–û–ë–ê–í–ò–¢–¨:
1. **–í–≤–µ–¥–µ–Ω–∏–µ** (2-3 –∞–±–∑–∞—Ü–∞, ~150-200 —Å–ª–æ–≤):
   - –û —á–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –∏ –µ–≥–æ –∑–Ω–∞—á–∏–º–æ—Å—Ç—å
   - –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–º—ã, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç —Ä–∞—Å–∫—Ä—ã—Ç—ã
   - –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ä–µ—Ñ–µ—Ä–∞—Ç–∞
   
2. **–ó–∞–∫–ª—é—á–µ–Ω–∏–µ** (2-3 –∞–±–∑–∞—Ü–∞, ~150-200 —Å–ª–æ–≤):
   - –û—Å–Ω–æ–≤–Ω—ã–µ –≤—ã–≤–æ–¥—ã –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞
   - –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
   - –ò—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞

–†–ï–§–ï–†–ê–¢ (—Å–æ—Ö—Ä–∞–Ω–∏ –µ–≥–æ –ü–û–õ–ù–û–°–¢–¨–Æ):
{base_referat}

–î–û–ë–ê–í–¨ –í–í–ï–î–ï–ù–ò–ï –ò –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï (–∏—Ç–æ–≥–æ –º–∏–Ω–∏–º—É–º {min_final_words} —Å–ª–æ–≤):"""
            
            print(f"[RAG_MANAGER] Adding introduction and conclusion")
            final_referat = await llm.get_response("", final_prompt)
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —ç—Ç–∞–ø–∞
            final_words = len(final_referat.split())
            print(f"[RAG_MANAGER] Final referat with intro/conclusion:")
            print(f"  - Base: {base_words} words")
            print(f"  - Final: {final_words} words")
            print(f"  - Added: {final_words - base_words} words")
            
            if final_words < min_final_words:
                print(f"  ‚ö†Ô∏è  WARNING: Final output is too short ({final_words} < {min_final_words})")
        else:
            final_referat = referat_parts[0]
            final_words = len(final_referat.split())
            print(f"[RAG_MANAGER] Single-part referat (no merging needed)")
        
        # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_chars = sum(len(chunk['content']) for chunk in chunks)
        total_input_words = sum(len(chunk['content'].split()) for chunk in chunks)
        final_compression = (final_words / total_input_words * 100) if total_input_words > 0 else 0
        
        print(f"\n[RAG_MANAGER] ========== REFERAT STATISTICS ==========")
        print(f"Original document:")
        print(f"  - Total chars: {total_chars}")
        print(f"  - Total words: {total_input_words}")
        print(f"  - Chunks: {len(chunks)}")
        print(f"Referat:")
        print(f"  - Total chars: {len(final_referat)}")
        print(f"  - Total words: {final_words}")
        print(f"  - Compression ratio: {final_compression:.1f}% (target: 30-40%)")
        print(f"==========================================")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º PDF
        pdf_generator = get_pdf_generator()
        
        # –°–æ–∑–¥–∞–µ–º URL-–±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞ –¥–ª—è PDF
        base_filename = os.path.splitext(document['filename'])[0]
        # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã, –∑–∞–º–µ–Ω—è–µ–º –Ω–∞ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏—è
        safe_filename = re.sub(r'[^\w\-.]', '_', base_filename)
        # –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏—è
        safe_filename = re.sub(r'_+', '_', safe_filename)
        # –£–±–∏—Ä–∞–µ–º –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏—è –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ
        safe_filename = safe_filename.strip('_')
        pdf_filename = f"{safe_filename}_referat.pdf"
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º PDF
        pdf_path = pdf_generator.generate_referat_pdf(
            referat_text=final_referat,
            filename=pdf_filename,
            output_path=output_dir,
            original_filename=document['filename'],
            chunk_count=len(chunks),
            metadata=document.get('metadata', {})
        )
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º URL –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
        pdf_url = f"/referats/{pdf_filename}"
        
        print(f"[RAG_MANAGER] PDF generated: {pdf_path}")
        print(f"[RAG_MANAGER] PDF URL: {pdf_url}")
        print(f"[RAG_MANAGER] ========== CREATE REFERAT COMPLETE ==========\n")
        
        return {
            'referat': final_referat,
            'document_id': document_id,
            'filename': document['filename'],
            'chunk_count': len(chunks),
            'pdf_url': pdf_url,
            'pdf_path': pdf_path
        }

