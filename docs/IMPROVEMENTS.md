# üöÄ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–∏—Å–∫–∞

## ‚úÖ –£–∂–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ

### 1. **–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏**
- –î–æ–±–∞–≤–ª–µ–Ω –ø–æ—Ä–æ–≥ `min_similarity = 0.3` (30%)
- –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å similarity < 30% –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤—ã–≤–∞—é—Ç—Å—è
- –≠—Ç–æ —É–±–∏—Ä–∞–µ—Ç —Å–æ–≤—Å–µ–º –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏

### 2. **–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —á–∞–Ω–∫–∏–Ω–≥–∞**
- `CHUNK_SIZE=1200` - –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –±–æ–ª—å—à–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
- `CHUNK_OVERLAP=200` - —Ö–æ—Ä–æ—à–∞—è —Å–≤—è–∑–Ω–æ—Å—Ç—å –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
- `context_limit=7` - 7 —Å–∞–º—ã—Ö —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤

### 3. **–ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å embeddings**
- `paraphrase-multilingual-mpnet-base-v2` (768 –∏–∑–º–µ—Ä–µ–Ω–∏–π)
- –•–æ—Ä–æ—à–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
- –í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞

### 4. **–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã**
- LLM –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç—ã —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ –∏ —Å–ø–∏—Å–∫–∞–º–∏
- Markdown —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥ –Ω–∞ —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–µ
- –í—ã–¥–µ–ª–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤

## üéØ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è (–¥–ª—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏)

### 1. **–†–µ—Ä–∞–Ω–∫–∏–Ω–≥ (Reranking)** ‚≠ê –í—ã—Å–æ–∫–∏–π —ç—Ñ—Ñ–µ–∫—Ç
**–ß—Ç–æ —ç—Ç–æ:** –ü–æ—Å–ª–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è cross-encoder –¥–ª—è –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- –ü–æ–≤—ã—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ 20-30%
- Cross-encoder —É—á–∏—Ç—ã–≤–∞–µ—Ç –≤–∑–∞–∏–º–æ—Å–≤—è–∑—å –∑–∞–ø—Ä–æ—Å–∞ –∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
- –û—Å–æ–±–µ–Ω–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤

**–†–µ–∞–ª–∏–∑–∞—Ü–∏—è:**
```python
from sentence_transformers import CrossEncoder

class RAGManager:
    def __init__(self):
        # ...
        self.reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
    
    async def search_with_reranking(self, query, limit=5):
        # 1. –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ (–ø–æ–ª—É—á–∞–µ–º 20 –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤)
        candidates = await self.search(query, limit=20)
        
        # 2. –†–µ—Ä–∞–Ω–∫–∏–Ω–≥
        pairs = [[query, result['content']] for result in candidates]
        scores = self.reranker.predict(pairs)
        
        # 3. –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –Ω–æ–≤—ã–º —Å–∫–æ—Ä–∞–º
        for idx, result in enumerate(candidates):
            result['rerank_score'] = scores[idx]
        
        candidates.sort(key=lambda x: x['rerank_score'], reverse=True)
        return candidates[:limit]
```

### 2. **–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ (Hybrid Search)** ‚≠ê –°—Ä–µ–¥–Ω–∏–π —ç—Ñ—Ñ–µ–∫—Ç
**–ß—Ç–æ —ç—Ç–æ:** –ö–æ–º–±–∏–Ω–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ (—Å–µ–º–∞–Ω—Ç–∏–∫–∞) –∏ full-text search (–∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞).

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- –ù–∞—Ö–æ–¥–∏—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ —Ç–æ—á–Ω—ã–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è–º —Å–ª–æ–≤
- –î–æ–ø–æ–ª–Ω—è–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫
- –û—Å–æ–±–µ–Ω–Ω–æ –ø–æ–ª–µ–∑–µ–Ω –¥–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤

**–†–µ–∞–ª–∏–∑–∞—Ü–∏—è:**
```sql
-- –î–æ–±–∞–≤–∏—Ç—å –≤ init.sql
CREATE INDEX IF NOT EXISTS chunks_content_gin_idx ON chunks USING gin(to_tsvector('russian', content));

-- –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫
SELECT c.id, c.content, c.chunk_index, d.filename, d.id as document_id,
       -- –í–µ–∫—Ç–æ—Ä–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å
       1 - (c.embedding <=> $1::vector) as vector_similarity,
       -- Full-text –ø–æ–∏—Å–∫
       ts_rank(to_tsvector('russian', c.content), plainto_tsquery('russian', $2)) as text_rank,
       -- –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∫–æ—Ä
       (0.7 * (1 - (c.embedding <=> $1::vector)) + 0.3 * ts_rank(...)) as combined_score
FROM chunks c
JOIN documents d ON c.document_id = d.id
ORDER BY combined_score DESC
LIMIT $3
```

### 3. **Query Expansion (–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞)** ‚≠ê –°—Ä–µ–¥–Ω–∏–π —ç—Ñ—Ñ–µ–∫—Ç
**–ß—Ç–æ —ç—Ç–æ:** –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–∏–Ω–æ–Ω–∏–º–æ–≤ –∏ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –∫ –∑–∞–ø—Ä–æ—Å—É.

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- –ù–∞—Ö–æ–¥–∏—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –¥—Ä—É–≥–∏–º–∏ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞–º–∏
- –£–ª—É—á—à–∞–µ—Ç –ø–æ–ª–Ω–æ—Ç—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

**–†–µ–∞–ª–∏–∑–∞—Ü–∏—è:**
```python
async def expand_query(self, query: str) -> str:
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º LLM –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞
    expansion_prompt = f"""
    –ü–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–π –≤–æ–ø—Ä–æ—Å 3 —Ä–∞–∑–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏, 
    –∏—Å–ø–æ–ª—å–∑—É—è —Å–∏–Ω–æ–Ω–∏–º—ã –∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏:
    
    –í–æ–ø—Ä–æ—Å: {query}
    
    –ü–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∫–∏:
    1.
    2.
    3.
    """
    
    expanded = await self.llm.get_response("", expansion_prompt)
    return f"{query}\n{expanded}"
```

### 4. **–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π context_limit** ‚≠ê –ù–∏–∑–∫–∏–π —ç—Ñ—Ñ–µ–∫—Ç
**–ß—Ç–æ —ç—Ç–æ:** –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —á–∞–Ω–∫–æ–≤ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–∞.

**–†–µ–∞–ª–∏–∑–∞—Ü–∏—è:**
```python
def estimate_context_limit(self, query: str) -> int:
    # –ü—Ä–æ—Å—Ç—ã–µ –≤–æ–ø—Ä–æ—Å—ã - –º–µ–Ω—å—à–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    if len(query.split()) < 5:
        return 5
    # –°–ª–æ–∂–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã - –±–æ–ª—å—à–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    elif len(query.split()) > 15:
        return 10
    else:
        return 7
```

### 5. **Semantic Chunking** ‚≠ê –í—ã—Å–æ–∫–∏–π —ç—Ñ—Ñ–µ–∫—Ç
**–ß—Ç–æ —ç—Ç–æ:** –†–∞–∑–±–∏–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ —Å–º—ã—Å–ª–æ–≤—ã–º –≥—Ä–∞–Ω–∏—Ü–∞–º (–∞–±–∑–∞—Ü—ã, —Ä–∞–∑–¥–µ–ª—ã), –∞ –Ω–µ –ø–æ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É —Ä–∞–∑–º–µ—Ä—É.

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- –ß–∞–Ω–∫–∏ —Å–æ–¥–µ—Ä–∂–∞—Ç –∑–∞–∫–æ–Ω—á–µ–Ω–Ω—ã–µ –º—ã—Å–ª–∏
- –£–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
- –ú–µ–Ω—å—à–µ "–æ–±—Ä–µ–∑–∞–Ω–Ω—ã—Ö" –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π

**–†–µ–∞–ª–∏–∑–∞—Ü–∏—è:**
```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=1200,
    chunk_overlap=200,
    separators=["\n\n", "\n", ". ", " ", ""],
    length_function=len
)
```

### 6. **–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏** ‚≠ê –°—Ä–µ–¥–Ω–∏–π —ç—Ñ—Ñ–µ–∫—Ç
**–ß—Ç–æ —ç—Ç–æ:** –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∫ —á–∞–Ω–∫–∞–º (–¥–∞—Ç–∞, –∫–∞—Ç–µ–≥–æ—Ä–∏—è, —Ä–∞–∑–¥–µ–ª) –¥–ª—è —É–º–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏.

**–†–µ–∞–ª–∏–∑–∞—Ü–∏—è:**
```python
# –î–æ–±–∞–≤–∏—Ç—å –ø—Ä–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
metadata = {
    'section': '–ì–ª–∞–≤–∞ 3',
    'date': '2024-01-01',
    'category': '–±–∏–æ–ª–æ–≥–∏—è',
    'page_number': 42
}

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤ –ø–æ–∏—Å–∫–µ
WHERE c.metadata->>'category' = '–±–∏–æ–ª–æ–≥–∏—è'
AND (c.metadata->>'date')::date > '2023-01-01'
```

### 7. **–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∑–∞–ø—Ä–æ—Å–æ–≤** ‚≠ê –ù–∏–∑–∫–∏–π —ç—Ñ—Ñ–µ–∫—Ç (—Å–∫–æ—Ä–æ—Å—Ç—å)
**–ß—Ç–æ —ç—Ç–æ:** –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤ —á–∞—Å—Ç–æ –∑–∞–¥–∞–≤–∞–µ–º—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤.

**–†–µ–∞–ª–∏–∑–∞—Ü–∏—è:**
```python
from functools import lru_cache

@lru_cache(maxsize=100)
def get_query_embedding(self, query: str):
    return self.embedding_model.encode(query)
```

### 8. **–£–º–Ω—ã–π max_tokens –¥–ª—è LLM** ‚≠ê –°—Ä–µ–¥–Ω–∏–π —ç—Ñ—Ñ–µ–∫—Ç
**–ß—Ç–æ —ç—Ç–æ:** –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ max_tokens –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –æ–±—ä–µ–º–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.

**–†–µ–∞–ª–∏–∑–∞—Ü–∏—è:**
```python
# –ë–æ–ª—å—à–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ = –±–æ–ª—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –æ—Ç–≤–µ—Ç–∞
context_tokens = len(context) // 4  # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
max_tokens = min(2048, max(1024, context_tokens // 2))
```

## üìä –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏

### –í—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç (–±—ã—Å—Ç—Ä—ã–π —ç—Ñ—Ñ–µ–∫—Ç):
1. ‚úÖ **–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ relevance** - —É–∂–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ
2. **–†–µ—Ä–∞–Ω–∫–∏–Ω–≥** - +20-30% —Ç–æ—á–Ω–æ—Å—Ç–∏
3. **Semantic Chunking** - –ª—É—á—à–µ –∫–∞—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

### –°—Ä–µ–¥–Ω–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç:
4. **–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫** - –Ω–∞—Ö–æ–¥–∏—Ç –±–æ–ª—å—à–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
5. **Query Expansion** - —É–ª—É—á—à–∞–µ—Ç –ø–æ–ª–Ω–æ—Ç—É
6. **–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ** - —É–º–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è

### –ù–∏–∑–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç:
7. **–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π context_limit** - –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç
8. **–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ** - —É—Å–∫–æ—Ä–µ–Ω–∏–µ, –Ω–æ –Ω–µ —Ç–æ—á–Ω–æ—Å—Ç—å

## üéØ –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π –ø–ª–∞–Ω –¥–µ–π—Å—Ç–≤–∏–π

### –®–∞–≥ 1: –†–µ—Ä–∞–Ω–∫–∏–Ω–≥ (1-2 —á–∞—Å–∞)
```bash
pip install sentence-transformers
# –î–æ–±–∞–≤–∏—Ç—å CrossEncoder –≤ RAGManager
# –û–±–Ω–æ–≤–∏—Ç—å –º–µ—Ç–æ–¥ search
```

### –®–∞–≥ 2: Semantic Chunking (30 –º–∏–Ω—É—Ç)
```bash
pip install langchain-text-splitters
# –û–±–Ω–æ–≤–∏—Ç—å DocumentProcessor
```

### –®–∞–≥ 3: –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ (2-3 —á–∞—Å–∞)
```sql
-- –û–±–Ω–æ–≤–∏—Ç—å init.sql
-- –ò–∑–º–µ–Ω–∏—Ç—å SQL –∑–∞–ø—Ä–æ—Å—ã –≤ vector_store.py
```

## üìà –û–∂–∏–¥–∞–µ–º—ã–µ —É–ª—É—á—à–µ–Ω–∏—è

–° —Ç–µ–∫—É—â–∏–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏:
- **–¢–æ—á–Ω–æ—Å—Ç—å:** ~70-75%
- **–ü–æ–ª–Ω–æ—Ç–∞:** ~60-65%

–ü–æ—Å–ª–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞ + semantic chunking:
- **–¢–æ—á–Ω–æ—Å—Ç—å:** ~85-90%
- **–ü–æ–ª–Ω–æ—Ç–∞:** ~75-80%

–ü–æ—Å–ª–µ full —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ (–≤—Å–µ —É–ª—É—á—à–µ–Ω–∏—è):
- **–¢–æ—á–Ω–æ—Å—Ç—å:** ~90-95%
- **–ü–æ–ª–Ω–æ—Ç–∞:** ~85-90%

## üîß –¢–æ–Ω–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ —Ç–µ–∫—É—â–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

### –ï—Å–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–ª–∏—à–∫–æ–º –æ–±—â–∏–µ:
```python
min_similarity = 0.4  # –ü–æ–≤—ã—Å–∏—Ç—å –ø–æ—Ä–æ–≥ (—Å–µ–π—á–∞—Å 0.3)
context_limit = 5     # –£–º–µ–Ω—å—à–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤ (—Å–µ–π—á–∞—Å 7)
```

### –ï—Å–ª–∏ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞:
```python
CHUNK_SIZE = 1500      # –£–≤–µ–ª–∏—á–∏—Ç—å —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–æ–≤ (—Å–µ–π—á–∞—Å 1200)
context_limit = 10     # –ë–æ–ª—å—à–µ —á–∞–Ω–∫–æ–≤ (—Å–µ–π—á–∞—Å 7)
min_similarity = 0.25  # –°–Ω–∏–∑–∏—Ç—å –ø–æ—Ä–æ–≥ (—Å–µ–π—á–∞—Å 0.3)
```

### –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç—ã —Å–ª–∏—à–∫–æ–º –º–µ–¥–ª–µ–Ω–Ω—ã–µ:
```python
context_limit = 5      # –ú–µ–Ω—å—à–µ —á–∞–Ω–∫–æ–≤ (—Å–µ–π—á–∞—Å 7)
max_tokens = 512       # –ö–æ—Ä–æ—á–µ –æ—Ç–≤–µ—Ç—ã LLM (—Å–µ–π—á–∞—Å 1024)
```

## üìö –ü–æ–ª–µ–∑–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã

- [Sentence Transformers](https://www.sbert.net/)
- [LangChain Text Splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/)
- [pgvector Best Practices](https://github.com/pgvector/pgvector)
- [RAG Evaluation Metrics](https://docs.ragas.io/)

---

**–¢–µ–∫—É—â–∏–π —Å—Ç–∞—Ç—É—Å:** ‚úÖ –ë–∞–∑–æ–≤—ã–µ —É–ª—É—á—à–µ–Ω–∏—è —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω—ã, —Å–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é

